
Q-Learning para Super Mario World
=========

![Python](https://img.shields.io/badge/Python-14354C?style=for-the-badge&logo=python&logoColor=white)


Este projeto implementa uma solu√ß√£o de aprendizado por refor√ßo usando Q-Learning para treinar o Mario a jogar a fase Yoshi's Island 1 no jogo Super Mario World. Ele utiliza a biblioteca Retro para intera√ß√£o com o ambiente do jogo.

## üíª Tecnologias utilizadas

* **Python:** Linguagem de programa√ß√£o principal.
* **Gym:** Framework para cria√ß√£o de ambientes de jogos para aprendizado por refor√ßo.
* **Gym-Retro:** Extens√£o do Gym para interagir com jogos cl√°ssicos, como o Super Mario World.
* **NumPy:** Biblioteca para computa√ß√£o num√©rica.
* **Pyglet:** Biblioteca para cria√ß√£o de janelas e gr√°ficos.


## üîß Como executar

Essas instru√ß√µes permitir√£o que voc√™ obtenha uma c√≥pia do projeto em opera√ß√£o na sua m√°quina local para fins de desenvolvimento e teste.


### Pr√©-requisitos

√â necess√°rio, primeiramente, ter o Pyenv instalado para poder gerenciar diferentes vers√µes do Python. Se voc√™ ainda n√£o tem o Pyenv, instale-o de acordo com seu sistema operacional:
- Linux/MacOS: 
    Para estes sistemas operacionais, basta seguir o passo-a-passo do projeto do pyenv encontrado no seguinte link: 
    [`pyenv`](https://github.com/pyenv/pyenv/blob/master/README.md)
- Windows: 
    Como o pyenv n√£o √© suportado no Windows, √© recomendado utilizar o projeto 
    [`pyenv-win`](https://github.com/pyenv-win/pyenv-win) feito por @kirankotari.

### Instala√ß√£o das Depend√™ncias

- Instale a vers√£o do Python: Certifique-se de instalar a vers√£o correta do Python:
```
pyenv install 3.8
pyenv shell 3.8
```
- Clone o reposit√≥rio: Se voc√™ tiver acesso ao c√≥digo-fonte, clone o reposit√≥rio para o seu computador local:
``` 
git clone https://[seu_repositorio]/nomeProjeto.git
```
- Navegue at√© o diret√≥rio do projeto
- Crie e ative um ambiente virtual (opcional, mas recomendado):
```
python -m venv nomeAmbiente
source nomeAmbiente/bin/activate
```
- Instale as depend√™ncias (opcional):
```
pip install gym==0.21.0
pip install gym-retro
```


### Execu√ß√£o do Projeto

Caso o ambiente tenha sido corretamente configurado, basta executar o script de teste:

```
python mario_q_learning.py treinar
ou
python mario_q_learning.py jogar
```
Substitua teste.py pelo nome do seu script principal, se for diferente.

## üìã Implementa√ß√£o do Q-Learning
O c√≥digo implementa o algoritmo Q-learning para treinar um agente a jogar Super Mario World. A ideia central √© que o agente aprende a tomar as melhores decis√µes (a√ß√µes) em cada estado do jogo, maximizando a recompensa a longo prazo.

Pontos-chave da implementa√ß√£o:

- Tabela Q: A tabela Q armazena os valores esperados de cada a√ß√£o em cada estado.
- Ambiente: O ambiente √© simulado usando a biblioteca retro, permitindo que o agente interaja com o jogo Super Mario World.
- Ciclo de treinamento: O agente interage com o ambiente, executa a√ß√µes, recebe recompensas e atualiza a tabela Q de acordo com a equa√ß√£o de Bellman.
- Pol√≠tica Epsilon-Greedy: O agente utiliza uma pol√≠tica epsilon-greedy para explorar o espa√ßo de a√ß√µes e explorar as melhores a√ß√µes conhecidas.
- Representa√ß√£o de estados: O estado do jogo √© representado por um vetor de caracter√≠sticas, que inclui a posi√ß√£o do Mario e informa√ß√µes sobre o ambiente ao redor.
- Verifica√ß√£o da posi√ß√£o do Mario: O c√≥digo verifica se o Mario est√° no ch√£o, uma condi√ß√£o importante para o agente de aprendizado por refor√ßo. Ele transforma o estado do jogo em uma matriz e verifica se h√° um bloco s√≥lido abaixo do Mario. Se n√£o houver, o agente realiza a√ß√µes para fazer o Mario pular e esperar at√© que ele toque o ch√£o novamente. Essa parte do c√≥digo √© respons√°vel por melhorar significativamente o desempenho do agente.
- Recompensa: O sistema de recompensa incentiva o agente a avan√ßar no jogo, evitando quedas e colis√µes. Ele premia o agente por avan√ßar horizontalmente e penaliza por retroceder ou morrer. Al√©m disso, o sistema desincentiva o agente ficar parado na mesma posi√ß√£o horizontal, penalizando-o.

## üîç Resultados
Utilizando o m√©todo q-learning, encontramos alguns problemas para obter um agente eficaz, o agente consegue terminar a fase eventualmente,
mas n√£o fomos capazes de fazer com que ap√≥es treinado, ele sempre execute exatamente os mesmos passos. Na modelagem escolhida
ao terminar a fase, o agente voltar atribuindo uma pontua√ß√£o muito alta (5000) nas escolhas feitas na tentativa bem sucedida.
Por√©m ao reexecutar o agente, ele eventualmente acaba tomando caminhos diferentes (mesmo com o coeficiente de explora√ß√£o igualado a 0).
Acreditamos que o motivo disso seja por termos usado o state (array de 0, 1 e -1, contendo uma representa√ß√£o do que √© exibido na tela)
e os states nem sempre se repetem exatamente igual nas jogadas, ent√£o talvez seja necess√°rio um treinamento muito maior para acumular uma cole√ß√£o de jogadas que levam o agente a concluir a fase 